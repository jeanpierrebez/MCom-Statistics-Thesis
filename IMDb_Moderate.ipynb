{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loVnbXBNJJul"
      },
      "outputs": [],
      "source": [
        "# Installing the required dependencies\n",
        "!pip install datasets\n",
        "!pip install nltk\n",
        "!pip install bs4\n",
        "!pip install gensim\n",
        "!pip install nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading in the required libraries\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import tqdm as tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import numpy as np\n",
        "from imblearn.datasets import make_imbalance\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.under_sampling import TomekLinks  \n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.flow as nafc\n",
        "from nlpaug.util import Action\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import statistics\n",
        "from numpy import vstack"
      ],
      "metadata": {
        "id": "cm-NjB04JxxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the IMDb dataset from Hugging Face and creating the different splits\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "imdb_train_text = imdb_dataset[\"train\"][\"text\"]\n",
        "imdb_train_y = np.array(imdb_dataset[\"train\"][\"label\"])\n",
        "imdb_test_text = imdb_dataset[\"test\"][\"text\"]\n",
        "imdb_test_y = np.array(imdb_dataset[\"test\"][\"label\"])\n",
        "semi_size = random.sample(range(0, 50000), 10000)\n",
        "imdb_unlabeled = imdb_dataset[\"unsupervised\"].select(semi_size)[\"text\"]\n",
        "imdb_unlabeled_y = np.array(imdb_dataset[\"unsupervised\"].select(semi_size)[\"label\"])"
      ],
      "metadata": {
        "id": "RlbOQrkLKNFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulating the moderate imbalanced data scenario\n",
        "num_majority = int((len(imdb_train_text))/2)\n",
        "num_minority = int((25/75)*num_majority)\n",
        "imdb_text_array = np.asarray(imdb_train_text).reshape(-1, 1)\n",
        "imdb_text_moderate, imdb_y_moderate = make_imbalance(imdb_text_array, imdb_train_y, sampling_strategy = {0: num_minority, 1: num_majority}, random_state = 123)\n",
        "imdb_text_moderate = imdb_text_moderate.tolist()\n",
        "imdb_text_moderate = [''.join(ele) for ele in imdb_text_moderate]"
      ],
      "metadata": {
        "id": "rmhiQnTXOanv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing our training data\n",
        "stopwords_list = stopwords.words(\"english\")\n",
        "processed_reviews_train = []\n",
        "for review in tqdm.tqdm(imdb_train_text):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_train.append(temp_update)\n",
        "\n",
        "# Preprocessing our testing data\n",
        "processed_reviews_test = []\n",
        "for review in tqdm.tqdm(imdb_test_text):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_test.append(temp_update)\n",
        "\n",
        "# Preprocessing our imbalanced data set\n",
        "processed_reviews_imbalanced = []\n",
        "for review in tqdm.tqdm(imdb_text_moderate):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_imbalanced.append(temp_update)\n",
        "\n",
        "# Preprocessing our unlabeled data\n",
        "processed_reviews_unlabeled = []\n",
        "for review in tqdm.tqdm(imdb_unlabeled):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_unlabeled.append(temp_update)\n",
        "\n",
        "# Defining the final word2vec model parameters\n",
        "vec_size = 600\n",
        "window_size = 15\n",
        "model_architecture = 1\n",
        "subsample = 1e-2\n",
        "\n",
        "# Creating the document level representation using the final word2vec model for each review in the different data sets\n",
        "w2v_model = Word2Vec(sentences = processed_reviews_train, size = vec_size, window = window_size, sg = model_architecture, sample = subsample, seed = 123)\n",
        "\n",
        "imdb_imbalanced = np.zeros([len(processed_reviews_imbalanced), vec_size])\n",
        "for i in tqdm.tqdm(range(len(processed_reviews_imbalanced))):\n",
        "  word_list = []\n",
        "  for word in processed_reviews_imbalanced[i]:\n",
        "    if word in w2v_model.wv.vocab:\n",
        "      word_list.append(word)\n",
        "      imdb_imbalanced[i] = np.mean(w2v_model.wv[word_list], axis = 0)\n",
        "\n",
        "imdb_test = np.zeros([len(processed_reviews_test), vec_size])\n",
        "for i in tqdm.tqdm(range(len(processed_reviews_test))):\n",
        "  word_list = []\n",
        "  for word in processed_reviews_test[i]:\n",
        "    if word in w2v_model.wv.vocab:\n",
        "      word_list.append(word)\n",
        "      imdb_test[i] = np.mean(w2v_model.wv[word_list], axis = 0)\n",
        "\n",
        "imdb_unlabeled = np.zeros([len(processed_reviews_unlabeled), vec_size])\n",
        "for i in tqdm.tqdm(range(len(processed_reviews_unlabeled))):\n",
        "  word_list = []\n",
        "  for word in processed_reviews_unlabeled[i]:\n",
        "    if word in w2v_model.wv.vocab:\n",
        "      word_list.append(word)\n",
        "      imdb_unlabeled[i] = np.mean(w2v_model.wv[word_list], axis = 0)\n",
        "\n",
        "# Training the final Naive Bayes model on the imbalanced dataset\n",
        "imdb_nb = GaussianNB()\n",
        "imdb_nb_fit = imdb_nb.fit(imdb_imbalanced, imdb_y_moderate)\n",
        "\n",
        "# Training the final Logistic Regression model on the imbalanced dataset\n",
        "imdb_logreg = LogisticRegression(penalty = \"l2\", tol = 0.0001, C = 1, max_iter = 500, random_state = 123, solver = 'liblinear')\n",
        "imdb_logreg_fit = imdb_logreg.fit(imdb_imbalanced, imdb_y_moderate)\n",
        "\n",
        "# Training the final SVM model on the imbalanced dataset\n",
        "imdb_svm = SGDClassifier(alpha = 0.03125, random_state = 123)\n",
        "imdb_svm_fit = imdb_svm.fit(imdb_imbalanced, imdb_y_moderate)\n",
        "\n",
        "# Training the final Random Forests model on the imbalanced dataset\n",
        "imdb_rf = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2, max_features = 17, max_samples = 0.75, random_state = 123)\n",
        "imdb_rf_fit = imdb_rf.fit(imdb_imbalanced, imdb_y_moderate)\n",
        "\n",
        "# Using our final models to obtain predictions on the test and compute the F1-score\n",
        "imdb_nb_preds = imdb_nb_fit.predict(imdb_test)\n",
        "imdb_nb_f1 = f1_score(imdb_test_y, imdb_nb_preds)\n",
        "\n",
        "imdb_logreg_preds = imdb_logreg_fit.predict(imdb_test)\n",
        "imdb_logreg_f1 = f1_score(imdb_test_y, imdb_logreg_preds)\n",
        "\n",
        "imdb_svm_preds = imdb_svm_fit.predict(imdb_test)\n",
        "imdb_svm_f1 = f1_score(imdb_test_y, imdb_svm_preds)\n",
        "\n",
        "imdb_rf_preds = imdb_rf_fit.predict(imdb_test)\n",
        "imdb_rf_f1 = f1_score(imdb_test_y, imdb_rf_preds)\n",
        "\n",
        "# Outputting the F1-score for the final models\n",
        "print(f\"Naive Bayes F1-score: {imdb_nb_f1}\")\n",
        "print(f\"Logistic Regression F1-score: {imdb_logreg_f1}\")\n",
        "print(f\"SVM F1-score: {imdb_svm_f1}\")\n",
        "print(f\"Random Forest F1-score: {imdb_rf_f1}\")"
      ],
      "metadata": {
        "id": "YmA-EZs3aJil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random oversampling\n",
        "imdb_ros = RandomOverSampler(random_state = 123)\n",
        "imdb_text_resampled, imdb_label_resampled = imdb_ros.fit_resample(imdb_imbalanced, imdb_y_moderate)\n",
        "\n",
        "# Training the final Naive Bayes model on the balanced dataset\n",
        "imdb_nb = GaussianNB()\n",
        "imdb_nb_fit = imdb_nb.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Training the final Logistic Regression model on the balanced dataset\n",
        "imdb_logreg = LogisticRegression(penalty = \"l2\", tol = 0.0001, C = 1, max_iter = 500, random_state = 123, solver = 'liblinear')\n",
        "imdb_logreg_fit = imdb_logreg.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Training the final SVM model on the balanced dataset\n",
        "imdb_svm = SGDClassifier(alpha = 0.03125, random_state = 123)\n",
        "imdb_svm_fit = imdb_svm.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Training the final Random Forest model on the balanced dataset\n",
        "imdb_rf = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2, max_features = 17, max_samples = 0.75, random_state = 123)\n",
        "imdb_rf_fit = imdb_rf.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Using our final models to obtain predictions on the test set and compute the F1-score\n",
        "imdb_nb_preds = imdb_nb_fit.predict(imdb_test)\n",
        "imdb_nb_f1 = f1_score(imdb_test_y, imdb_nb_preds)\n",
        "\n",
        "imdb_logreg_preds = imdb_logreg_fit.predict(imdb_test)\n",
        "imdb_logreg_f1 = f1_score(imdb_test_y, imdb_logreg_preds)\n",
        "\n",
        "imdb_svm_preds = imdb_svm_fit.predict(imdb_test)\n",
        "imdb_svm_f1 = f1_score(imdb_test_y, imdb_svm_preds)\n",
        "\n",
        "imdb_rf_preds = imdb_rf_fit.predict(imdb_test)\n",
        "imdb_rf_f1 = f1_score(imdb_test_y, imdb_rf_preds)\n",
        "\n",
        "# Outputting the F1-score for the final models\n",
        "print(f\"Naive Bayes F1-score: {imdb_nb_f1}\")\n",
        "print(f\"Logistic Regression F1-score: {imdb_logreg_f1}\")\n",
        "print(f\"SVM F1-score: {imdb_svm_f1}\")\n",
        "print(f\"Random Forest F1-score: {imdb_rf_f1}\")"
      ],
      "metadata": {
        "id": "J2T4jSvUloXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyperparameter configurations\n",
        "neighbours = 10\n",
        "\n",
        "# Oversampling using Synthetic Minority Over-sampling Technique (SMOTE)\n",
        "imdb_smote = SMOTE(k_neighbors = neighbours, random_state = 123)\n",
        "imdb_text_resampled, imdb_label_resampled = imdb_smote.fit_resample(imdb_imbalanced, imdb_y_moderate)\n",
        "\n",
        "# Training a Naive Bayes model on the balanced dataset\n",
        "imdb_nb = GaussianNB()\n",
        "imdb_nb_fit = imdb_nb.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Training a Logistic Regression model on the balanced dataset\n",
        "imdb_logreg = LogisticRegression(penalty = \"l2\", tol = 0.0001, C = 1, max_iter = 500, random_state = 123, solver = 'liblinear')\n",
        "imdb_logreg_fit = imdb_logreg.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Training an SVM model on the balanced dataset\n",
        "imdb_svm = SGDClassifier(alpha = 0.03125, random_state = 123)\n",
        "imdb_svm_fit = imdb_svm.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Training a Random Forest model on the balanced dataset\n",
        "imdb_rf = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2, max_features = 17, max_samples = 0.75, random_state = 123)\n",
        "imdb_rf_fit = imdb_rf.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Using our final models to obtain predictions on the test set and compute the F1-score\n",
        "imdb_nb_preds = imdb_nb_fit.predict(imdb_test)\n",
        "imdb_nb_f1 = f1_score(imdb_test_y, imdb_nb_preds)\n",
        "\n",
        "imdb_logreg_preds = imdb_logreg_fit.predict(imdb_test)\n",
        "imdb_logreg_f1 = f1_score(imdb_test_y, imdb_logreg_preds)\n",
        "\n",
        "imdb_svm_preds = imdb_svm_fit.predict(imdb_test)\n",
        "imdb_svm_f1 = f1_score(imdb_test_y, imdb_svm_preds)\n",
        "\n",
        "imdb_rf_preds = imdb_rf_fit.predict(imdb_test)\n",
        "imdb_rf_f1 = f1_score(imdb_test_y, imdb_rf_preds)\n",
        "\n",
        "# Outputting the F1-score for the final models\n",
        "print(f\"Naive Bayes F1-score: {imdb_nb_f1}\")\n",
        "print(f\"Logistic Regression F1-score: {imdb_logreg_f1}\")\n",
        "print(f\"SVM F1-score: {imdb_svm_f1}\")\n",
        "print(f\"Random Forest F1-score: {imdb_rf_f1}\")"
      ],
      "metadata": {
        "id": "w6ClgSqPaLMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random undersampling\n",
        "imdb_rus = RandomUnderSampler(random_state = 123)\n",
        "imdb_text_resampled, imdb_label_resampled = imdb_rus.fit_resample(imdb_imbalanced, imdb_y_moderate)\n",
        "\n",
        "# Training the final Naive Bayes model on the balanced dataset\n",
        "imdb_nb = GaussianNB()\n",
        "imdb_nb_fit = imdb_nb.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Training the final Logistic Regression model on the balanced dataset\n",
        "imdb_logreg = LogisticRegression(penalty = \"l2\", tol = 0.0001, C = 1, max_iter = 500, random_state = 123, solver = 'liblinear')\n",
        "imdb_logreg_fit = imdb_logreg.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Training the final SVM model on the balanced dataset\n",
        "imdb_svm = SGDClassifier(alpha = 0.03125, random_state = 123)\n",
        "imdb_svm_fit = imdb_svm.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Training the final Random Forest model on the balanced dataset\n",
        "imdb_rf = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2, max_features = 17, max_samples = 0.75, random_state = 123)\n",
        "imdb_rf_fit = imdb_rf.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Using our final models to obtain predictions on the test set and compute the F1-score\n",
        "imdb_nb_preds = imdb_nb_fit.predict(imdb_test)\n",
        "imdb_nb_f1 = f1_score(imdb_test_y, imdb_nb_preds)\n",
        "\n",
        "imdb_logreg_preds = imdb_logreg_fit.predict(imdb_test)\n",
        "imdb_logreg_f1 = f1_score(imdb_test_y, imdb_logreg_preds)\n",
        "\n",
        "imdb_svm_preds = imdb_svm_fit.predict(imdb_test)\n",
        "imdb_svm_f1 = f1_score(imdb_test_y, imdb_svm_preds)\n",
        "\n",
        "imdb_rf_preds = imdb_rf_fit.predict(imdb_test)\n",
        "imdb_rf_f1 = f1_score(imdb_test_y, imdb_rf_preds)\n",
        "\n",
        "# Outputting the F1-score for the final models\n",
        "print(f\"Naive Bayes F1-score: {imdb_nb_f1}\")\n",
        "print(f\"Logistic Regression F1-score: {imdb_logreg_f1}\")\n",
        "print(f\"SVM F1-score: {imdb_svm_f1}\")\n",
        "print(f\"Random Forest F1-score: {imdb_rf_f1}\")"
      ],
      "metadata": {
        "id": "-6RUq6pjo7qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Undersampling using Tomek's Links\n",
        "imdb_tl = TomekLinks()\n",
        "imdb_text_resampled, imdb_label_resampled = imdb_tl.fit_resample(imdb_imbalanced, imdb_y_moderate)\n",
        "\n",
        "# Training the final Naive Bayes model on the balanced dataset\n",
        "imdb_nb = GaussianNB()\n",
        "imdb_nb_fit = imdb_nb.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Training the final Logistic Regression model on the balanced dataset\n",
        "imdb_logreg = LogisticRegression(penalty = \"l2\", tol = 0.0001, C = 1, max_iter = 500, random_state = 123, solver = 'liblinear')\n",
        "imdb_logreg_fit = imdb_logreg.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Training the final SVM model on the balanced dataset\n",
        "imdb_svm = SGDClassifier(alpha = 0.03125, random_state = 123)\n",
        "imdb_svm_fit = imdb_svm.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Training the final Random Forest model on the balanced dataset\n",
        "imdb_rf = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2, max_features = 17, max_samples = 0.75, random_state = 123)\n",
        "imdb_rf_fit = imdb_rf.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "# Using our final models to obtain predictions on the test set and compute the F1-score\n",
        "imdb_nb_preds = imdb_nb_fit.predict(imdb_test)\n",
        "imdb_nb_f1 = f1_score(imdb_test_y, imdb_nb_preds)\n",
        "\n",
        "imdb_logreg_preds = imdb_logreg_fit.predict(imdb_test)\n",
        "imdb_logreg_f1 = f1_score(imdb_test_y, imdb_logreg_preds)\n",
        "\n",
        "imdb_svm_preds = imdb_svm_fit.predict(imdb_test)\n",
        "imdb_svm_f1 = f1_score(imdb_test_y, imdb_svm_preds)\n",
        "\n",
        "imdb_rf_preds = imdb_rf_fit.predict(imdb_test)\n",
        "imdb_rf_f1 = f1_score(imdb_test_y, imdb_rf_preds)\n",
        "\n",
        "# Outputting the F1-score for the final models\n",
        "print(f\"Naive Bayes F1-score: {imdb_nb_f1}\")\n",
        "print(f\"Logistic Regression F1-score: {imdb_logreg_f1}\")\n",
        "print(f\"SVM F1-score: {imdb_svm_f1}\")\n",
        "print(f\"Random Forest F1-score: {imdb_rf_f1}\")"
      ],
      "metadata": {
        "id": "4tqJ09Sppc94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing our data for augmentation (random substitution)\n",
        "augment_index = np.array(imdb_text_moderate)[imdb_y_moderate == 0]\n",
        "augment_index = augment_index.tolist()\n",
        "augment_index = [''.join(ele) for ele in augment_index]\n",
        "\n",
        "# Preprocessing our data\n",
        "processed_reviews_aug = []\n",
        "for review in tqdm.tqdm(augment_index):\n",
        "    raw = BeautifulSoup(review)\n",
        "    html_remove = raw.get_text()\n",
        "    processed_reviews_aug.append(html_remove)\n",
        "\n",
        "# Defining the hyperparameter configuration\n",
        "aug_perc = 0.1\n",
        "num_augment = 2\n",
        "\n",
        "# Data augmentation (random subsitution)\n",
        "aug = naw.SynonymAug(aug_p = aug_perc, stopwords = stopwords_list)\n",
        "augmented = []\n",
        "for i in [1, num_augment]:\n",
        "  for review in augment_index:\n",
        "    temp = aug.augment(review)\n",
        "    augmented.append(temp)\n",
        "    augmented = [''.join(ele) for ele in augmented]\n",
        "\n",
        "# Preprocessing our augmented data\n",
        "processed_reviews_aug_post = []\n",
        "for review in tqdm.tqdm(augmented):\n",
        "  temp = simple_preprocess(review)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_aug_post.append(temp_update)\n",
        "\n",
        "# Creating the document level representation using the final word2vec model for each review in the augmented data set\n",
        "imdb_train_aug = np.zeros([len(processed_reviews_aug_post), vec_size])\n",
        "for i in tqdm.tqdm(range(len(processed_reviews_aug_post))):\n",
        "  word_list = []\n",
        "  for word in processed_reviews_aug_post[i]:\n",
        "    if word in w2v_model.wv.vocab:\n",
        "      word_list.append(word)\n",
        "      imdb_train_aug[i] = np.mean(w2v_model.wv[word_list], axis = 0)\n",
        "\n",
        "# Combining the imbalanced and augmented data sets\n",
        "imdb_train_aug_combined = vstack((imdb_imbalanced, imdb_train_aug))\n",
        "imdb_y_aug = np.zeros(shape=(len(processed_reviews_aug_post), ))\n",
        "imdb_y_aug_combined = np.concatenate((imdb_y_moderate, imdb_y_aug))\n",
        "\n",
        "# Training the final Naive Bayes model on the dataset\n",
        "imdb_nb = GaussianNB()\n",
        "imdb_nb_fit = imdb_nb.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "# Training the final Logistic Regression model on the dataset\n",
        "imdb_logreg = LogisticRegression(penalty = \"l2\", tol = 0.0001, C = 1, max_iter = 500, random_state = 123, solver = 'liblinear')\n",
        "imdb_logreg_fit = imdb_logreg.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "# Training the final SVM model on the dataset\n",
        "imdb_svm = SGDClassifier(alpha = 0.03125, random_state = 123)\n",
        "imdb_svm_fit = imdb_svm.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "# Training the final Random Forests model on the dataset\n",
        "imdb_rf = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2, max_features = 17, max_samples = 0.75, random_state = 123)\n",
        "imdb_rf_fit = imdb_rf.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "# Using our models to obtain predictions and compute the F1-score\n",
        "imdb_nb_preds = imdb_nb_fit.predict(imdb_test)\n",
        "imdb_nb_f1 = f1_score(imdb_test_y, imdb_nb_preds)\n",
        "\n",
        "imdb_logreg_preds = imdb_logreg_fit.predict(imdb_test)\n",
        "imdb_logreg_f1 = f1_score(imdb_test_y, imdb_logreg_preds)\n",
        "\n",
        "imdb_svm_preds = imdb_svm_fit.predict(imdb_test)\n",
        "imdb_svm_f1 = f1_score(imdb_test_y, imdb_svm_preds)\n",
        "\n",
        "imdb_rf_preds = imdb_rf_fit.predict(imdb_test)\n",
        "imdb_rf_f1 = f1_score(imdb_test_y, imdb_rf_preds)\n",
        "\n",
        "# Outputting the F1-score for the final models\n",
        "print(f\"Naive Bayes F1-score: {imdb_nb_f1}\")\n",
        "print(f\"Logistic Regression F1-score: {imdb_logreg_f1}\")\n",
        "print(f\"SVM F1-score: {imdb_svm_f1}\")\n",
        "print(f\"Random Forest F1-score: {imdb_rf_f1}\")"
      ],
      "metadata": {
        "id": "RAtPKg8gZInD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyperparameter configuration\n",
        "aug_perc = 0.2\n",
        "num_augment = 2\n",
        "\n",
        "# Data augmentation (random swap)\n",
        "aug = naw.RandomWordAug(action=\"swap\", aug_p = aug_perc, stopwords = stopwords_list)\n",
        "augmented = []\n",
        "for i in [1, num_augment]:\n",
        "  for review in augment_index:\n",
        "    temp = aug.augment(review)\n",
        "    augmented.append(temp)\n",
        "    augmented = [''.join(ele) for ele in augmented]\n",
        "\n",
        "# Preprocessing our augmented data\n",
        "processed_reviews_aug_post = []\n",
        "for review in tqdm.tqdm(augmented):\n",
        "  temp = simple_preprocess(review)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_aug_post.append(temp_update)\n",
        "\n",
        "# Creating the document level representation using the final word2vec model for each review in the augmented data set\n",
        "imdb_train_aug = np.zeros([len(processed_reviews_aug_post), vec_size])\n",
        "for i in tqdm.tqdm(range(len(processed_reviews_aug_post))):\n",
        "  word_list = []\n",
        "  for word in processed_reviews_aug_post[i]:\n",
        "    if word in w2v_model.wv.vocab:\n",
        "      word_list.append(word)\n",
        "      imdb_train_aug[i] = np.mean(w2v_model.wv[word_list], axis = 0)\n",
        "\n",
        "# Combining the imbalanced and augmented data sets\n",
        "imdb_train_aug_combined = vstack((imdb_imbalanced, imdb_train_aug))\n",
        "imdb_y_aug = np.zeros(shape=(len(processed_reviews_aug_post), ))\n",
        "imdb_y_aug_combined = np.concatenate((imdb_y_moderate, imdb_y_aug))\n",
        "\n",
        "# Training the final Naive Bayes model on the dataset\n",
        "imdb_nb = GaussianNB()\n",
        "imdb_nb_fit = imdb_nb.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "# Training the final Logistic Regression model on the dataset\n",
        "imdb_logreg = LogisticRegression(penalty = \"l2\", tol = 0.0001, C = 1, max_iter = 500, random_state = 123, solver = 'liblinear')\n",
        "imdb_logreg_fit = imdb_logreg.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "# Training the final SVM model on the dataset\n",
        "imdb_svm = SGDClassifier(alpha = 0.03125, random_state = 123)\n",
        "imdb_svm_fit = imdb_svm.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "# Training the final Random Forests model on the dataset\n",
        "imdb_rf = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2, max_features = 17, max_samples = 0.75, random_state = 123)\n",
        "imdb_rf_fit = imdb_rf.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "# Using our models to obtain predictions and compute the F1-score\n",
        "imdb_nb_preds = imdb_nb_fit.predict(imdb_test)\n",
        "imdb_nb_f1 = f1_score(imdb_test_y, imdb_nb_preds)\n",
        "\n",
        "imdb_logreg_preds = imdb_logreg_fit.predict(imdb_test)\n",
        "imdb_logreg_f1 = f1_score(imdb_test_y, imdb_logreg_preds)\n",
        "\n",
        "imdb_svm_preds = imdb_svm_fit.predict(imdb_test)\n",
        "imdb_svm_f1 = f1_score(imdb_test_y, imdb_svm_preds)\n",
        "\n",
        "imdb_rf_preds = imdb_rf_fit.predict(imdb_test)\n",
        "imdb_rf_f1 = f1_score(imdb_test_y, imdb_rf_preds)\n",
        "\n",
        "# Outputting the F1-score for the final models\n",
        "print(f\"Naive Bayes F1-score: {imdb_nb_f1}\")\n",
        "print(f\"Logistic Regression F1-score: {imdb_logreg_f1}\")\n",
        "print(f\"SVM F1-score: {imdb_svm_f1}\")\n",
        "print(f\"Random Forest F1-score: {imdb_rf_f1}\")"
      ],
      "metadata": {
        "id": "X9AXXMfQTxgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyperparameter configuration\n",
        "aug_perc = 0.2\n",
        "num_augment = 2\n",
        "\n",
        "# Data augmentation (random swap)\n",
        "aug = naw.RandomWordAug(action=\"delete\", aug_p = aug_perc, stopwords = stopwords_list)\n",
        "augmented = []\n",
        "for i in [1, num_augment]:\n",
        "  for review in augment_index:\n",
        "    temp = aug.augment(review)\n",
        "    augmented.append(temp)\n",
        "    augmented = [''.join(ele) for ele in augmented]\n",
        "\n",
        "# Preprocessing our augmented data\n",
        "processed_reviews_aug_post = []\n",
        "for review in tqdm.tqdm(augmented):\n",
        "  temp = simple_preprocess(review)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_aug_post.append(temp_update)\n",
        "\n",
        "# Creating the document level representation using the final word2vec model for each review in the augmented data set\n",
        "imdb_train_aug = np.zeros([len(processed_reviews_aug_post), vec_size])\n",
        "for i in tqdm.tqdm(range(len(processed_reviews_aug_post))):\n",
        "  word_list = []\n",
        "  for word in processed_reviews_aug_post[i]:\n",
        "    if word in w2v_model.wv.vocab:\n",
        "      word_list.append(word)\n",
        "      imdb_train_aug[i] = np.mean(w2v_model.wv[word_list], axis = 0)\n",
        "\n",
        "# Combining the imbalanced and augmented data sets\n",
        "imdb_train_aug_combined = vstack((imdb_imbalanced, imdb_train_aug))\n",
        "imdb_y_aug = np.zeros(shape=(len(processed_reviews_aug_post), ))\n",
        "imdb_y_aug_combined = np.concatenate((imdb_y_moderate, imdb_y_aug))\n",
        "\n",
        "# Training the final Naive Bayes model on the dataset\n",
        "imdb_nb = GaussianNB()\n",
        "imdb_nb_fit = imdb_nb.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "# Training the final Logistic Regression model on the dataset\n",
        "imdb_logreg = LogisticRegression(penalty = \"l2\", tol = 0.0001, C = 1, max_iter = 500, random_state = 123, solver = 'liblinear')\n",
        "imdb_logreg_fit = imdb_logreg.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "# Training the final SVM model on the dataset\n",
        "imdb_svm = SGDClassifier(alpha = 0.03125, random_state = 123)\n",
        "imdb_svm_fit = imdb_svm.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "# Training the final Random Forests model on the dataset\n",
        "imdb_rf = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2, max_features = 17, max_samples = 0.75, random_state = 123)\n",
        "imdb_rf_fit = imdb_rf.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "# Using our models to obtain predictions and compute the F1-score\n",
        "imdb_nb_preds = imdb_nb_fit.predict(imdb_test)\n",
        "imdb_nb_f1 = f1_score(imdb_test_y, imdb_nb_preds)\n",
        "\n",
        "imdb_logreg_preds = imdb_logreg_fit.predict(imdb_test)\n",
        "imdb_logreg_f1 = f1_score(imdb_test_y, imdb_logreg_preds)\n",
        "\n",
        "imdb_svm_preds = imdb_svm_fit.predict(imdb_test)\n",
        "imdb_svm_f1 = f1_score(imdb_test_y, imdb_svm_preds)\n",
        "\n",
        "imdb_rf_preds = imdb_rf_fit.predict(imdb_test)\n",
        "imdb_rf_f1 = f1_score(imdb_test_y, imdb_rf_preds)\n",
        "\n",
        "# Outputting the F1-score for the final models\n",
        "print(f\"Naive Bayes F1-score: {imdb_nb_f1}\")\n",
        "print(f\"Logistic Regression F1-score: {imdb_logreg_f1}\")\n",
        "print(f\"SVM F1-score: {imdb_svm_f1}\")\n",
        "print(f\"Random Forest F1-score: {imdb_rf_f1}\")"
      ],
      "metadata": {
        "id": "jOHFVIdkT6Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing our data\n",
        "imdb_text_semi = vstack((imdb_imbalanced, imdb_unlabeled))\n",
        "imdb_y_semi = np.concatenate((imdb_y_moderate, imdb_unlabeled_y))\n",
        "\n",
        "# Defining the hyperparameter configuration\n",
        "thresh = 0.99\n",
        "\n",
        "# Implementing the self-training algorithm using the final Naive Bayes model as the base classifer\n",
        "imdb_nb = GaussianNB()\n",
        "imdb_nb_semi = SelfTrainingClassifier(base_estimator = imdb_nb, threshold = thresh)\n",
        "imdb_nb_semi_fit = imdb_nb_semi.fit(imdb_text_semi, imdb_y_semi)\n",
        "\n",
        "# Implementing the self-training algorithm using the final Logistic Regression model as the base classifer\n",
        "imdb_logreg = LogisticRegression(penalty = \"l2\", tol = 0.0001, C = 1, max_iter = 500, random_state = 123, solver = 'liblinear')\n",
        "imdb_logreg_semi = SelfTrainingClassifier(base_estimator = imdb_logreg, threshold = thresh)\n",
        "imdb_logreg_semi_fit = imdb_logreg_semi.fit(imdb_text_semi, imdb_y_semi)\n",
        "\n",
        "# Implementing the self-training algorithm using the final Random Forests model as the base classifer\n",
        "imdb_rf = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2, max_features = 17, max_samples = 0.75, random_state = 123)\n",
        "imdb_rf_semi = SelfTrainingClassifier(base_estimator = imdb_rf, threshold = thresh)\n",
        "imdb_rf_semi_fit = imdb_rf_semi.fit(imdb_text_semi, imdb_y_semi)\n",
        "\n",
        "# Using our models to obtain predictions and compute the F1-score\n",
        "imdb_nb_semi_preds = imdb_nb_semi_fit.predict(imdb_test)\n",
        "imdb_nb_f1 = f1_score(imdb_test_y, imdb_nb_semi_preds)\n",
        "\n",
        "imdb_logreg_semi_preds = imdb_logreg_semi_fit.predict(imdb_test)\n",
        "imdb_logreg_f1 = f1_score(imdb_test_y, imdb_logreg_semi_preds)\n",
        "\n",
        "imdb_rf_semi_preds = imdb_rf_semi_fit.predict(imdb_test)\n",
        "imdb_rf_f1 = f1_score(imdb_test_y, imdb_rf_semi_preds)\n",
        "\n",
        "# Outputting the F1-score for the final models\n",
        "print(f\"Naive Bayes F1-score: {imdb_nb_f1}\")\n",
        "print(f\"Logistic Regression F1-score: {imdb_logreg_f1}\")\n",
        "print(f\"Random Forest F1-score: {imdb_rf_f1}\")"
      ],
      "metadata": {
        "id": "cOLb3wf6tXeX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}