{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ojp3fNH-uJY"
      },
      "outputs": [],
      "source": [
        "# Installing the required dependencies\n",
        "!pip install datasets\n",
        "!pip install nltk\n",
        "!pip install bs4\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading in the required libraries\n",
        "from datasets import load_dataset\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import tqdm as tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import statistics"
      ],
      "metadata": {
        "id": "Y0pCcDsg-_Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the IMDb dataset from Hugging Face and creating the training and development splits\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "imdb_train_full = imdb_dataset[\"train\"]\n",
        "imdb_train_dev = imdb_train_full.train_test_split(test_size = 0.2, stratify_by_column = \"label\", seed = 123)\n",
        "imdb_train_text = imdb_train_dev[\"train\"][\"text\"]\n",
        "imdb_train_y = np.array(imdb_train_dev[\"train\"][\"label\"])\n",
        "imdb_dev_text = imdb_train_dev[\"test\"][\"text\"]\n",
        "imdb_dev_y = np.array(imdb_train_dev[\"test\"][\"label\"])"
      ],
      "metadata": {
        "id": "XeL-lxo0_FxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing our training data (pre-split)\n",
        "stopwords_list = stopwords.words(\"english\")\n",
        "processed_reviews_train = []\n",
        "for review in tqdm.tqdm(imdb_train_full[\"text\"]):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_train.append(temp_update)\n",
        "\n",
        "# Preprocessing our training data (post-split)\n",
        "stopwords_list = stopwords.words(\"english\")\n",
        "processed_reviews_train_post = []\n",
        "for review in tqdm.tqdm(imdb_train_text):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_train_post.append(temp_update)\n",
        "\n",
        "# Preprocessing our development data\n",
        "processed_reviews_dev = []\n",
        "for review in tqdm.tqdm(imdb_dev_text):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_dev.append(temp_update)\n",
        "\n",
        "# Defining the hyperparameter configurations\n",
        "vec_size = [100, 300, 600]\n",
        "window_size = [3, 7, 12, 15]\n",
        "model_architecture = [0, 1]\n",
        "subsample = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
        "num_models = len(vec_size)*len(window_size)*len(model_architecture)*len(subsample)\n",
        "models = np.zeros(shape=(num_models, 6))\n",
        "\n",
        "# word2vec hyperparameter tuning\n",
        "iteration = 0\n",
        "for v in vec_size:\n",
        "  for w in window_size:\n",
        "    for m in model_architecture:\n",
        "      for s in subsample:\n",
        "        # Creating the document level representation using word2vec for each review in the train and development splits\n",
        "        w2v_model = Word2Vec(sentences = processed_reviews_train, size = v, window = w, sg = m, sample = s, seed = 123)\n",
        "\n",
        "        imdb_train_post = np.zeros([len(processed_reviews_train_post), v])\n",
        "        for i in tqdm.tqdm(range(len(processed_reviews_train_post))):\n",
        "          word_list = []\n",
        "          for word in processed_reviews_train_post[i]:\n",
        "            if word in w2v_model.wv.vocab:\n",
        "              word_list.append(word)\n",
        "              imdb_train_post[i] = np.mean(w2v_model.wv[word_list], axis = 0)\n",
        "\n",
        "        imdb_dev = np.zeros([len(processed_reviews_dev), v])\n",
        "        for i in tqdm.tqdm(range(len(processed_reviews_dev))):\n",
        "          word_list = []\n",
        "          for word in processed_reviews_dev[i]:\n",
        "            if word in w2v_model.wv.vocab:\n",
        "              word_list.append(word)\n",
        "              imdb_dev[i] = np.mean(w2v_model.wv[word_list], axis = 0)\n",
        "\n",
        "        # Training a Naive Bayes model on the datasets\n",
        "        imdb_nb = GaussianNB()\n",
        "        imdb_nb_fit = imdb_nb.fit(imdb_train_post, imdb_train_y)\n",
        "\n",
        "        # Training a Logistic Regression model on the datasets\n",
        "        imdb_logreg = LogisticRegression(random_state = 123, solver = 'liblinear')\n",
        "        imdb_logreg_fit = imdb_logreg.fit(imdb_train_post, imdb_train_y)\n",
        "\n",
        "        # Training an SVM model on the datasets\n",
        "        imdb_svm = SGDClassifier(random_state = 123)\n",
        "        imdb_svm_fit = imdb_svm.fit(imdb_train_post, imdb_train_y)\n",
        "\n",
        "        # Training a Random Forests model on the datasets\n",
        "        imdb_rf = RandomForestClassifier(random_state = 123)\n",
        "        imdb_rf_fit = imdb_rf.fit(imdb_train_post, imdb_train_y)\n",
        "\n",
        "        # Using our models to obtain predictions and compute the F1-score\n",
        "        imdb_nb_preds = imdb_nb_fit.predict(imdb_dev)\n",
        "        imdb_nb_f1 = f1_score(imdb_dev_y, imdb_nb_preds)\n",
        "\n",
        "        imdb_logreg_preds = imdb_logreg_fit.predict(imdb_dev)\n",
        "        imdb_logreg_f1 = f1_score(imdb_dev_y, imdb_logreg_preds)\n",
        "\n",
        "        imdb_svm_preds = imdb_svm_fit.predict(imdb_dev)\n",
        "        imdb_svm_f1 = f1_score(imdb_dev_y, imdb_svm_preds)\n",
        "\n",
        "        imdb_rf_preds = imdb_rf_fit.predict(imdb_dev)\n",
        "        imdb_rf_f1 = f1_score(imdb_dev_y, imdb_rf_preds)\n",
        "\n",
        "        med_f1 = statistics.median([imdb_nb_f1, imdb_logreg_f1, imdb_svm_f1, imdb_rf_f1])\n",
        "\n",
        "        # Updating our model matrix\n",
        "        models[iteration] = [iteration, v, w, m, s, med_f1]\n",
        "        print(models[iteration])\n",
        "        iteration = iteration + 1"
      ],
      "metadata": {
        "id": "4f0VvI0aAd_N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}