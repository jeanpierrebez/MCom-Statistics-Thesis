{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duCoS5jyNuNc"
      },
      "outputs": [],
      "source": [
        "# Installing the required dependencies\n",
        "!pip install datasets\n",
        "!pip install nltk\n",
        "!pip install bs4\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading in the required libraries\n",
        "from datasets import load_dataset\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import tqdm as tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import statistics"
      ],
      "metadata": {
        "id": "5a167AIEN2Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the IMDb dataset from Hugging Face and creating the training and development splits\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "imdb_train_full = imdb_dataset[\"train\"]\n",
        "imdb_train_dev = imdb_train_full.train_test_split(test_size = 0.2, stratify_by_column = \"label\", seed = 123)\n",
        "imdb_train_text = imdb_train_dev[\"train\"][\"text\"]\n",
        "imdb_train_y = np.array(imdb_train_dev[\"train\"][\"label\"])\n",
        "imdb_dev_text = imdb_train_dev[\"test\"][\"text\"]\n",
        "imdb_dev_y = np.array(imdb_train_dev[\"test\"][\"label\"])"
      ],
      "metadata": {
        "id": "nM1LY03IN69E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing our training data (pre-split)\n",
        "stopwords_list = stopwords.words(\"english\")\n",
        "processed_reviews_train = []\n",
        "for review in tqdm.tqdm(imdb_train_full[\"text\"]):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_train.append(temp_update)\n",
        "\n",
        "# Preprocessing our training data (post-split)\n",
        "stopwords_list = stopwords.words(\"english\")\n",
        "processed_reviews_train_post = []\n",
        "for review in tqdm.tqdm(imdb_train_text):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_train_post.append(temp_update)\n",
        "\n",
        "# Preprocessing our development data\n",
        "processed_reviews_dev = []\n",
        "for review in tqdm.tqdm(imdb_dev_text):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_dev.append(temp_update)\n",
        "\n",
        "# Defining the final word2vec model parameters\n",
        "vec_size = 600\n",
        "window_size = 15\n",
        "model_architecture = 1\n",
        "subsample = 1e-2\n",
        "\n",
        "# Creating the document level representation using the final word2vec model for each review in the training (post-split) and development splits\n",
        "w2v_model = Word2Vec(sentences = processed_reviews_train, size = vec_size, window = window_size, sg = model_architecture, sample = subsample, seed = 123)\n",
        "\n",
        "imdb_train_post = np.zeros([len(processed_reviews_train_post), vec_size])\n",
        "for i in tqdm.tqdm(range(len(processed_reviews_train_post))):\n",
        "  word_list = []\n",
        "  for word in processed_reviews_train_post[i]:\n",
        "    if word in w2v_model.wv.vocab:\n",
        "      word_list.append(word)\n",
        "      imdb_train_post[i] = np.mean(w2v_model.wv[word_list], axis = 0)\n",
        "\n",
        "imdb_dev = np.zeros([len(processed_reviews_dev), vec_size])\n",
        "for i in tqdm.tqdm(range(len(processed_reviews_dev))):\n",
        "  word_list = []\n",
        "  for word in processed_reviews_dev[i]:\n",
        "    if word in w2v_model.wv.vocab:\n",
        "      word_list.append(word)\n",
        "      imdb_dev[i] = np.mean(w2v_model.wv[word_list], axis = 0)      "
      ],
      "metadata": {
        "id": "ZoIh709kN8DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyperparameter configurations\n",
        "penalty_term = [\"l1\", \"l2\"]\n",
        "tolerance = [1e-6, 1e-4, 1e-2]\n",
        "regularisation = [1/10, 1/8, 1/6, 1/4, 1/2, 1]\n",
        "max_iterations = [100, 300, 500]\n",
        "num_models = len(penalty_term)*len(tolerance)*len(regularisation)*len(max_iterations)\n",
        "models = np.zeros(shape=(num_models, 5))\n",
        "\n",
        "# Logistic regression hyperparameter tuning\n",
        "iteration = 0\n",
        "for p in penalty_term:\n",
        "  for t in tolerance:\n",
        "    for r in regularisation:\n",
        "      for m in max_iterations:\n",
        "        # Training a Logistic Regression model on the datasets\n",
        "        imdb_logreg = LogisticRegression(penalty = p, tol = t, C = r, max_iter = m, random_state = 123, solver = 'liblinear')\n",
        "        imdb_logreg_fit = imdb_logreg.fit(imdb_train_post, imdb_train_y)\n",
        "\n",
        "        # Using our models to obtain predictions and compute the F1-score\n",
        "        imdb_logreg_preds = imdb_logreg_fit.predict(imdb_dev)\n",
        "        imdb_logreg_f1 = f1_score(imdb_dev_y, imdb_logreg_preds)\n",
        "\n",
        "        # Updating our model matrix\n",
        "        models[iteration] = [iteration, t, r, m, imdb_logreg_f1]\n",
        "        print(models[iteration])\n",
        "        iteration = iteration + 1"
      ],
      "metadata": {
        "id": "mcNyiDoHSYga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyperparameter configurations\n",
        "regularisation = [0.03125, 0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]\n",
        "num_models = len(regularisation)\n",
        "models = np.zeros(shape=(num_models, 3))\n",
        "\n",
        "# SVM hyperparameter tuning\n",
        "iteration = 0\n",
        "for r in regularisation:\n",
        "  # Training an SVM model on the datasets\n",
        "  imdb_svm = SGDClassifier(alpha = r, random_state = 123)\n",
        "  imdb_svm_fit = imdb_svm.fit(imdb_train_post, imdb_train_y)\n",
        "\n",
        "  # Using our models to obtain predictions and compute the F1-score\n",
        "  imdb_svm_preds = imdb_svm_fit.predict(imdb_dev)\n",
        "  imdb_svm_f1 = f1_score(imdb_dev_y, imdb_svm_preds)\n",
        "\n",
        "  # Updating our model matrix\n",
        "  models[iteration] = [iteration, r, imdb_svm_f1]\n",
        "  print(models[iteration])\n",
        "  iteration = iteration + 1"
      ],
      "metadata": {
        "id": "yENMC-5sU9o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyperparameter configurations\n",
        "num_trees = [100, 300, 500]\n",
        "min_samples = [2, 5, 10, 20]\n",
        "num_features = [1, 5, 17]\n",
        "num_samples = [0.5, 0.75]\n",
        "num_models = len(num_trees)*len(min_samples)*len(num_features)*len(num_samples)\n",
        "models = np.zeros(shape=(num_models, 6))\n",
        "\n",
        "# Random forests hyperparameter tuning\n",
        "iteration = 0\n",
        "for t in num_trees:\n",
        "  for ns in num_samples:\n",
        "    for f in num_features:\n",
        "      for ms in min_samples:\n",
        "        # Training a Random Forests model on the datasets\n",
        "        imdb_rf = RandomForestClassifier(n_estimators = t, min_samples_leaf = ms, max_features = f, max_samples = ns, random_state = 123)\n",
        "        imdb_rf_fit = imdb_rf.fit(imdb_train_post, imdb_train_y)\n",
        "\n",
        "        # Using our models to obtain predictions and compute the F1-score\n",
        "        imdb_rf_preds = imdb_rf_fit.predict(imdb_dev)\n",
        "        imdb_rf_f1 = f1_score(imdb_dev_y, imdb_rf_preds)\n",
        "\n",
        "        # Updating our model matrix\n",
        "        models[iteration] = [iteration, t, ns, f, ms, imdb_rf_f1]\n",
        "        print(models[iteration])\n",
        "        iteration = iteration + 1"
      ],
      "metadata": {
        "id": "4tw0mrm4VGwH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}