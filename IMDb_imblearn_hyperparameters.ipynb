{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loVnbXBNJJul"
      },
      "outputs": [],
      "source": [
        "# Installing the required dependencies\n",
        "!pip install datasets\n",
        "!pip install nltk\n",
        "!pip install bs4\n",
        "!pip install gensim\n",
        "!pip install nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading in the required libraries\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import tqdm as tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import numpy as np\n",
        "from imblearn.datasets import make_imbalance\n",
        "from imblearn.over_sampling import SMOTE \n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.flow as nafc\n",
        "from nlpaug.util import Action\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import statistics\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from numpy import vstack"
      ],
      "metadata": {
        "id": "cm-NjB04JxxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the IMDb dataset from Hugging Face and creating the different splits\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "imdb_train_full = imdb_dataset[\"train\"]\n",
        "imdb_train_dev = imdb_train_full.train_test_split(test_size = 0.2, stratify_by_column = \"label\", seed = 123)\n",
        "imdb_train_text = imdb_train_dev[\"train\"][\"text\"]\n",
        "imdb_train_y = np.array(imdb_train_dev[\"train\"][\"label\"])\n",
        "imdb_dev_text = imdb_train_dev[\"test\"][\"text\"]\n",
        "imdb_dev_y = np.array(imdb_train_dev[\"test\"][\"label\"])\n",
        "semi_size = random.sample(range(0, 50000), 10000)\n",
        "imdb_unlabeled = imdb_dataset[\"unsupervised\"].select(semi_size)[\"text\"]\n",
        "imdb_unlabeled_y = np.array(imdb_dataset[\"unsupervised\"].select(semi_size)[\"label\"])"
      ],
      "metadata": {
        "id": "RlbOQrkLKNFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulating the moderate imbalanced data scenario\n",
        "num_majority = int((len(imdb_train_text))/2)\n",
        "num_minority = int((25/75)*num_majority)\n",
        "imdb_text_array = np.asarray(imdb_train_text).reshape(-1, 1)\n",
        "imdb_text_moderate, imdb_y_moderate = make_imbalance(imdb_text_array, imdb_train_y, sampling_strategy = {0: num_minority, 1: num_majority}, random_state = 123)\n",
        "imdb_text_moderate = imdb_text_moderate.tolist()\n",
        "imdb_text_moderate = [''.join(ele) for ele in imdb_text_moderate]"
      ],
      "metadata": {
        "id": "rmhiQnTXOanv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing our training data (pre-split)\n",
        "stopwords_list = stopwords.words(\"english\")\n",
        "processed_reviews_train = []\n",
        "for review in tqdm.tqdm(imdb_train_full[\"text\"]):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_train.append(temp_update)\n",
        "\n",
        "# Preprocessing our training data (post-split)\n",
        "processed_reviews_train_post = []\n",
        "for review in tqdm.tqdm(imdb_train_text):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_train_post.append(temp_update)\n",
        "\n",
        "# Preprocessing our imbalanced data set\n",
        "processed_reviews_imbalanced = []\n",
        "for review in tqdm.tqdm(imdb_text_moderate):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_imbalanced.append(temp_update)\n",
        "\n",
        "# Preprocessing our development data\n",
        "processed_reviews_dev = []\n",
        "for review in tqdm.tqdm(imdb_dev_text):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_dev.append(temp_update)\n",
        "\n",
        "# Preprocessing our unlabeled data\n",
        "processed_reviews_unlabeled = []\n",
        "for review in tqdm.tqdm(imdb_unlabeled):\n",
        "  raw = BeautifulSoup(review)\n",
        "  html_remove = raw.get_text()\n",
        "  temp = simple_preprocess(html_remove)\n",
        "  temp_update = [x for x in temp if x not in stopwords_list]\n",
        "  processed_reviews_unlabeled.append(temp_update)\n",
        "\n",
        "# Defining the final word2vec model parameters\n",
        "vec_size = 600\n",
        "window_size = 15\n",
        "model_architecture = 1\n",
        "subsample = 1e-2\n",
        "\n",
        "# Creating the document level representation using the final word2vec model for each review in the different data sets\n",
        "w2v_model = Word2Vec(sentences = processed_reviews_train, size = vec_size, window = window_size, sg = model_architecture, sample = subsample, seed = 123)\n",
        "\n",
        "imdb_imbalanced = np.zeros([len(processed_reviews_imbalanced), vec_size])\n",
        "for i in tqdm.tqdm(range(len(processed_reviews_imbalanced))):\n",
        "  word_list = []\n",
        "  for word in processed_reviews_imbalanced[i]:\n",
        "    if word in w2v_model.wv.vocab:\n",
        "      word_list.append(word)\n",
        "      imdb_imbalanced[i] = np.mean(w2v_model.wv[word_list], axis = 0)\n",
        "\n",
        "imdb_dev = np.zeros([len(processed_reviews_dev), vec_size])\n",
        "for i in tqdm.tqdm(range(len(processed_reviews_dev))):\n",
        "  word_list = []\n",
        "  for word in processed_reviews_dev[i]:\n",
        "    if word in w2v_model.wv.vocab:\n",
        "      word_list.append(word)\n",
        "      imdb_dev[i] = np.mean(w2v_model.wv[word_list], axis = 0)\n",
        "\n",
        "imdb_unlabeled = np.zeros([len(processed_reviews_unlabeled), vec_size])\n",
        "for i in tqdm.tqdm(range(len(processed_reviews_unlabeled))):\n",
        "  word_list = []\n",
        "  for word in processed_reviews_unlabeled[i]:\n",
        "    if word in w2v_model.wv.vocab:\n",
        "      word_list.append(word)\n",
        "      imdb_unlabeled[i] = np.mean(w2v_model.wv[word_list], axis = 0)"
      ],
      "metadata": {
        "id": "YmA-EZs3aJil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyperparameter configurations\n",
        "neighbours = [item for item in range(3, 21)]\n",
        "num_models = len(neighbours)\n",
        "models = np.zeros(shape=(num_models, 3))\n",
        "\n",
        "# SMOTE hyperparameter tuning\n",
        "iteration = 0\n",
        "for n in neighbours:\n",
        "  # Over-sampling using Synthetic Minority Over-sampling Technique (SMOTE)\n",
        "  imdb_smote = SMOTE(k_neighbors = n, random_state = 123)\n",
        "  imdb_text_resampled, imdb_label_resampled = imdb_smote.fit_resample(imdb_imbalanced, imdb_y_moderate)\n",
        "\n",
        "  # Training a Naive Bayes model on the dataset\n",
        "  imdb_nb = GaussianNB()\n",
        "  imdb_nb_fit = imdb_nb.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "  # Training a Logistic Regression model on the dataset\n",
        "  imdb_logreg = LogisticRegression(penalty = \"l2\", tol = 0.0001, C = 1, max_iter = 500, random_state = 123, solver = 'liblinear')\n",
        "  imdb_logreg_fit = imdb_logreg.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "  # Training an SVM model on the dataset\n",
        "  imdb_svm = SGDClassifier(alpha = 0.03125, random_state = 123)\n",
        "  imdb_svm_fit = imdb_svm.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "  # Training a Random Forests model on the dataset\n",
        "  imdb_rf = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2, max_features = 17, max_samples = 0.75, random_state = 123)\n",
        "  imdb_rf_fit = imdb_rf.fit(imdb_text_resampled, imdb_label_resampled)\n",
        "\n",
        "  # Using our models to obtain predictions and compute the F1-score\n",
        "  imdb_nb_preds = imdb_nb_fit.predict(imdb_dev)\n",
        "  imdb_nb_f1 = f1_score(imdb_dev_y, imdb_nb_preds)\n",
        "\n",
        "  imdb_logreg_preds = imdb_logreg_fit.predict(imdb_dev)\n",
        "  imdb_logreg_f1 = f1_score(imdb_dev_y, imdb_logreg_preds)\n",
        "\n",
        "  imdb_svm_preds = imdb_svm_fit.predict(imdb_dev)\n",
        "  imdb_svm_f1 = f1_score(imdb_dev_y, imdb_svm_preds)\n",
        "\n",
        "  imdb_rf_preds = imdb_rf_fit.predict(imdb_dev)\n",
        "  imdb_rf_f1 = f1_score(imdb_dev_y, imdb_rf_preds)\n",
        "\n",
        "  med_f1 = statistics.median([imdb_nb_f1, imdb_logreg_f1, imdb_svm_f1, imdb_rf_f1])\n",
        "\n",
        "  # Updating our model matrix\n",
        "  models[iteration] = [iteration, n, med_f1]\n",
        "  print(models[iteration])\n",
        "  iteration = iteration + 1"
      ],
      "metadata": {
        "id": "w6ClgSqPaLMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing our data for augmentation (random substitution)\n",
        "augment_index = np.array(imdb_text_moderate)[imdb_y_moderate == 0]\n",
        "augment_index = augment_index.tolist()\n",
        "augment_index = [''.join(ele) for ele in augment_index]\n",
        "\n",
        "# Preprocessing our data\n",
        "processed_reviews_aug = []\n",
        "for review in tqdm.tqdm(augment_index):\n",
        "    raw = BeautifulSoup(review)\n",
        "    html_remove = raw.get_text()\n",
        "    processed_reviews_aug.append(html_remove)\n",
        "\n",
        "# Defining the hyperparameter configuration\n",
        "aug_perc = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "num_augment = 2\n",
        "num_models = len(aug_perc)\n",
        "models = np.zeros(shape=(num_models, 3))\n",
        "\n",
        "# Data augmentation (random substitution hyperparameter tuning)\n",
        "iteration = 0\n",
        "for a in aug_perc:\n",
        "  # Data augmentation (random subsitution)\n",
        "  aug = naw.SynonymAug(aug_p = a, stopwords = stopwords_list)\n",
        "  augmented = []\n",
        "  for i in [1, num_augment]:\n",
        "    for review in augment_index:\n",
        "      temp = aug.augment(review)\n",
        "      augmented.append(temp)\n",
        "  augmented = [''.join(ele) for ele in augmented]\n",
        "\n",
        "  # Preprocessing our augmented data\n",
        "  processed_reviews_aug_post = []\n",
        "  for review in tqdm.tqdm(augmented):\n",
        "    temp = simple_preprocess(review)\n",
        "    temp_update = [x for x in temp if x not in stopwords_list]\n",
        "    processed_reviews_aug_post.append(temp_update)\n",
        "\n",
        "  # Creating the document level representation using the final word2vec model for each review in the augmented data set\n",
        "  imdb_train_aug = np.zeros([len(processed_reviews_aug_post), vec_size])\n",
        "  for i in tqdm.tqdm(range(len(processed_reviews_aug_post))):\n",
        "    word_list = []\n",
        "    for word in processed_reviews_aug_post[i]:\n",
        "      if word in w2v_model.wv.vocab:\n",
        "        word_list.append(word)\n",
        "        imdb_train_aug[i] = np.mean(w2v_model.wv[word_list], axis = 0)\n",
        "\n",
        "  # Combining the imbalanced and augmented data sets\n",
        "  imdb_train_aug_combined = vstack((imdb_imbalanced, imdb_train_aug))\n",
        "  imdb_y_aug = np.zeros(shape=(len(processed_reviews_aug_post), ))\n",
        "  imdb_y_aug_combined = np.concatenate((imdb_y_moderate, imdb_y_aug))\n",
        "\n",
        "  # Training a Naive Bayes model on the dataset\n",
        "  imdb_nb = GaussianNB()\n",
        "  imdb_nb_fit = imdb_nb.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "  # Training a Logistic Regression model on the dataset\n",
        "  imdb_logreg = LogisticRegression(penalty = \"l2\", tol = 0.0001, C = 1, max_iter = 500, random_state = 123, solver = 'liblinear')\n",
        "  imdb_logreg_fit = imdb_logreg.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "  # Training an SVM model on the dataset\n",
        "  imdb_svm = SGDClassifier(alpha = 0.03125, random_state = 123)\n",
        "  imdb_svm_fit = imdb_svm.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "  # Training a Random Forests model on the dataset\n",
        "  imdb_rf = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2, max_features = 17, max_samples = 0.75, random_state = 123)\n",
        "  imdb_rf_fit = imdb_rf.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "  # Using our models to obtain predictions and compute the F1-score\n",
        "  imdb_nb_preds = imdb_nb_fit.predict(imdb_dev)\n",
        "  imdb_nb_f1 = f1_score(imdb_dev_y, imdb_nb_preds)\n",
        "\n",
        "  imdb_logreg_preds = imdb_logreg_fit.predict(imdb_dev)\n",
        "  imdb_logreg_f1 = f1_score(imdb_dev_y, imdb_logreg_preds)\n",
        "\n",
        "  imdb_svm_preds = imdb_svm_fit.predict(imdb_dev)\n",
        "  imdb_svm_f1 = f1_score(imdb_dev_y, imdb_svm_preds)\n",
        "\n",
        "  imdb_rf_preds = imdb_rf_fit.predict(imdb_dev)\n",
        "  imdb_rf_f1 = f1_score(imdb_dev_y, imdb_rf_preds)\n",
        "\n",
        "  med_f1 = statistics.median([imdb_nb_f1, imdb_logreg_f1, imdb_svm_f1, imdb_rf_f1])\n",
        "\n",
        "  # Updating our model matrix\n",
        "  models[iteration] = [iteration, a, med_f1]\n",
        "  print(models[iteration])\n",
        "  iteration = iteration + 1"
      ],
      "metadata": {
        "id": "RAtPKg8gZInD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyperparameter configuration\n",
        "aug_perc = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "num_augment = 2\n",
        "num_models = len(aug_perc)\n",
        "models = np.zeros(shape=(num_models, 3))\n",
        "\n",
        "# Data augmentation (random swap hyperparameter tuning)\n",
        "iteration = 0\n",
        "for a in aug_perc:\n",
        "  # Data augmentation (random swap)\n",
        "  aug = naw.RandomWordAug(action=\"swap\", aug_p = a, stopwords = stopwords_list)\n",
        "  augmented = []\n",
        "  for i in [1, num_augment]:\n",
        "    for review in augment_index:\n",
        "      temp = aug.augment(review)\n",
        "      augmented.append(temp)\n",
        "  augmented = [''.join(ele) for ele in augmented]\n",
        "\n",
        "  # Preprocessing our augmented data\n",
        "  processed_reviews_aug_post = []\n",
        "  for review in tqdm.tqdm(augmented):\n",
        "    temp = simple_preprocess(review)\n",
        "    temp_update = [x for x in temp if x not in stopwords_list]\n",
        "    processed_reviews_aug_post.append(temp_update)\n",
        "\n",
        "  # Creating the document level representation using the final word2vec model for each review in the augmented data set\n",
        "  imdb_train_aug = np.zeros([len(processed_reviews_aug_post), vec_size])\n",
        "  for i in tqdm.tqdm(range(len(processed_reviews_aug_post))):\n",
        "    word_list = []\n",
        "    for word in processed_reviews_aug_post[i]:\n",
        "      if word in w2v_model.wv.vocab:\n",
        "        word_list.append(word)\n",
        "        imdb_train_aug[i] = np.mean(w2v_model.wv[word_list], axis = 0)\n",
        "\n",
        "  # Combining the imbalanced and augmented data sets\n",
        "  imdb_train_aug_combined = vstack((imdb_imbalanced, imdb_train_aug))\n",
        "  imdb_y_aug = np.zeros(shape=(len(processed_reviews_aug_post), ))\n",
        "  imdb_y_aug_combined = np.concatenate((imdb_y_moderate, imdb_y_aug))\n",
        "\n",
        "  # Training a Naive Bayes model on the dataset\n",
        "  imdb_nb = GaussianNB()\n",
        "  imdb_nb_fit = imdb_nb.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "  # Training a Logistic Regression model on the dataset\n",
        "  imdb_logreg = LogisticRegression(penalty = \"l2\", tol = 0.0001, C = 1, max_iter = 500, random_state = 123, solver = 'liblinear')\n",
        "  imdb_logreg_fit = imdb_logreg.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "  # Training an SVM model on the dataset\n",
        "  imdb_svm = SGDClassifier(alpha = 0.03125, random_state = 123)\n",
        "  imdb_svm_fit = imdb_svm.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "  # Training a Random Forests model on the dataset\n",
        "  imdb_rf = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2, max_features = 17, max_samples = 0.75, random_state = 123)\n",
        "  imdb_rf_fit = imdb_rf.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "  # Using our models to obtain predictions and compute the F1-score\n",
        "  imdb_nb_preds = imdb_nb_fit.predict(imdb_dev)\n",
        "  imdb_nb_f1 = f1_score(imdb_dev_y, imdb_nb_preds)\n",
        "\n",
        "  imdb_logreg_preds = imdb_logreg_fit.predict(imdb_dev)\n",
        "  imdb_logreg_f1 = f1_score(imdb_dev_y, imdb_logreg_preds)\n",
        "\n",
        "  imdb_svm_preds = imdb_svm_fit.predict(imdb_dev)\n",
        "  imdb_svm_f1 = f1_score(imdb_dev_y, imdb_svm_preds)\n",
        "\n",
        "  imdb_rf_preds = imdb_rf_fit.predict(imdb_dev)\n",
        "  imdb_rf_f1 = f1_score(imdb_dev_y, imdb_rf_preds)\n",
        "\n",
        "  med_f1 = statistics.median([imdb_nb_f1, imdb_logreg_f1, imdb_svm_f1, imdb_rf_f1])\n",
        "\n",
        "  # Updating our model matrix\n",
        "  models[iteration] = [iteration, a, med_f1]\n",
        "  print(models[iteration])\n",
        "  iteration = iteration + 1"
      ],
      "metadata": {
        "id": "X9AXXMfQTxgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyperparameter configuration\n",
        "aug_perc = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "num_augment = 2\n",
        "num_models = len(aug_perc)\n",
        "models = np.zeros(shape=(num_models, 3))\n",
        "\n",
        "# Data augmentation (random deletion hyperparameter tuning)\n",
        "iteration = 0\n",
        "for a in aug_perc:\n",
        "  # Data augmentation (random deletion)\n",
        "  aug = naw.RandomWordAug(action=\"delete\", aug_p = a, stopwords = stopwords_list)\n",
        "  augmented = []\n",
        "  for i in [1, num_augment]:\n",
        "    for review in augment_index:\n",
        "      temp = aug.augment(review)\n",
        "      augmented.append(temp)\n",
        "  augmented = [''.join(ele) for ele in augmented]\n",
        "\n",
        "  # Preprocessing our augmented data\n",
        "  processed_reviews_aug_post = []\n",
        "  for review in tqdm.tqdm(augmented):\n",
        "    temp = simple_preprocess(review)\n",
        "    temp_update = [x for x in temp if x not in stopwords_list]\n",
        "    processed_reviews_aug_post.append(temp_update)\n",
        "\n",
        "  # Creating the document level representation using the final word2vec model for each review in the augmented data set\n",
        "  imdb_train_aug = np.zeros([len(processed_reviews_aug_post), vec_size])\n",
        "  for i in tqdm.tqdm(range(len(processed_reviews_aug_post))):\n",
        "    word_list = []\n",
        "    for word in processed_reviews_aug_post[i]:\n",
        "      if word in w2v_model.wv.vocab:\n",
        "        word_list.append(word)\n",
        "        imdb_train_aug[i] = np.mean(w2v_model.wv[word_list], axis = 0)\n",
        "\n",
        "  # Combining the imbalanced and augmented data sets\n",
        "  imdb_train_aug_combined = vstack((imdb_imbalanced, imdb_train_aug))\n",
        "  imdb_y_aug = np.zeros(shape=(len(processed_reviews_aug_post), ))\n",
        "  imdb_y_aug_combined = np.concatenate((imdb_y_moderate, imdb_y_aug))\n",
        "\n",
        "  # Training a Naive Bayes model on the dataset\n",
        "  imdb_nb = GaussianNB()\n",
        "  imdb_nb_fit = imdb_nb.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "  # Training a Logistic Regression model on the dataset\n",
        "  imdb_logreg = LogisticRegression(penalty = \"l2\", tol = 0.0001, C = 1, max_iter = 500, random_state = 123, solver = 'liblinear')\n",
        "  imdb_logreg_fit = imdb_logreg.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "  # Training an SVM model on the dataset\n",
        "  imdb_svm = SGDClassifier(alpha = 0.03125, random_state = 123)\n",
        "  imdb_svm_fit = imdb_svm.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "  # Training a Random Forests model on the dataset\n",
        "  imdb_rf = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2, max_features = 17, max_samples = 0.75, random_state = 123)\n",
        "  imdb_rf_fit = imdb_rf.fit(imdb_train_aug_combined, imdb_y_aug_combined)\n",
        "\n",
        "  # Using our models to obtain predictions and compute the F1-score\n",
        "  imdb_nb_preds = imdb_nb_fit.predict(imdb_dev)\n",
        "  imdb_nb_f1 = f1_score(imdb_dev_y, imdb_nb_preds)\n",
        "\n",
        "  imdb_logreg_preds = imdb_logreg_fit.predict(imdb_dev)\n",
        "  imdb_logreg_f1 = f1_score(imdb_dev_y, imdb_logreg_preds)\n",
        "\n",
        "  imdb_svm_preds = imdb_svm_fit.predict(imdb_dev)\n",
        "  imdb_svm_f1 = f1_score(imdb_dev_y, imdb_svm_preds)\n",
        "\n",
        "  imdb_rf_preds = imdb_rf_fit.predict(imdb_dev)\n",
        "  imdb_rf_f1 = f1_score(imdb_dev_y, imdb_rf_preds)\n",
        "\n",
        "  med_f1 = statistics.median([imdb_nb_f1, imdb_logreg_f1, imdb_svm_f1, imdb_rf_f1])\n",
        "\n",
        "  # Updating our model matrix\n",
        "  models[iteration] = [iteration, a, med_f1]\n",
        "  print(models[iteration])\n",
        "  iteration = iteration + 1"
      ],
      "metadata": {
        "id": "jOHFVIdkT6Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing our data\n",
        "imdb_text_semi = vstack((imdb_imbalanced, imdb_unlabeled))\n",
        "imdb_y_semi = np.concatenate((imdb_y_moderate, imdb_unlabeled_y))\n",
        "\n",
        "# Defining the hyperparameter configurations\n",
        "thresh = [0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
        "num_models = len(thresh)\n",
        "models = np.zeros(shape=(num_models, 3))\n",
        "\n",
        "# Self-training hyperparameter tuning\n",
        "iteration = 0\n",
        "for t in thresh:\n",
        "  # Training a Naive Bayes model as the base classifer\n",
        "  imdb_nb = GaussianNB()\n",
        "  imdb_nb_semi = SelfTrainingClassifier(base_estimator = imdb_nb, threshold = t)\n",
        "  imdb_nb_semi_fit = imdb_nb_semi.fit(imdb_text_semi, imdb_y_semi)\n",
        "\n",
        "  # Training a Logistic Regression model as the base classifer\n",
        "  imdb_logreg = LogisticRegression(penalty = \"l2\", tol = 0.0001, C = 1, max_iter = 500, random_state = 123, solver = 'liblinear')\n",
        "  imdb_logreg_semi = SelfTrainingClassifier(base_estimator = imdb_logreg, threshold = t)\n",
        "  imdb_logreg_semi_fit = imdb_logreg_semi.fit(imdb_text_semi, imdb_y_semi)\n",
        "\n",
        "  # Training a Random Forests model as the base classifer\n",
        "  imdb_rf = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2, max_features = 17, max_samples = 0.75, random_state = 123)\n",
        "  imdb_rf_semi = SelfTrainingClassifier(base_estimator = imdb_rf, threshold = t)\n",
        "  imdb_rf_semi_fit = imdb_rf_semi.fit(imdb_text_semi, imdb_y_semi)\n",
        "\n",
        "  # Using our models to obtain predictions and compute the F1-score\n",
        "  imdb_nb_semi_preds = imdb_nb_semi_fit.predict(imdb_dev)\n",
        "  imdb_nb_f1 = f1_score(imdb_dev_y, imdb_nb_semi_preds)\n",
        "\n",
        "  imdb_logreg_semi_preds = imdb_logreg_semi_fit.predict(imdb_dev)\n",
        "  imdb_logreg_f1 = f1_score(imdb_dev_y, imdb_logreg_semi_preds)\n",
        "\n",
        "  imdb_rf_semi_preds = imdb_rf_semi_fit.predict(imdb_dev)\n",
        "  imdb_rf_f1 = f1_score(imdb_dev_y, imdb_rf_semi_preds)\n",
        "\n",
        "  med_f1 = statistics.median([imdb_nb_f1, imdb_logreg_f1, imdb_rf_f1])\n",
        "\n",
        "  # Updating our model matrix\n",
        "  models[iteration] = [iteration, t, med_f1]\n",
        "  print(models[iteration])\n",
        "  iteration = iteration + 1"
      ],
      "metadata": {
        "id": "cOLb3wf6tXeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyperparameter configurations\n",
        "nu_values = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "num_models = len(nu_values)\n",
        "models = np.zeros(shape=(num_models, 3))\n",
        "\n",
        "# One-class SVM hyperparameter tuning\n",
        "iteration = 0\n",
        "for n in nu_values:\n",
        "  # Training the one-class SVM model\n",
        "  imdb_oneclass_svm = OneClassSVM(kernel = \"linear\", nu = n)\n",
        "  imdb_oneclass_train = imdb_imbalanced[imdb_y_moderate == 1]\n",
        "  imdb_oneclass_model = imdb_oneclass_svm.fit(imdb_oneclass_train)\n",
        "\n",
        "  # Using our model to obtain predictions and compute the F1-score\n",
        "  imdb_oneclass_preds = imdb_oneclass_model.predict(imdb_dev)\n",
        "  imdb_oneclass_dev_y = imdb_dev_y\n",
        "  imdb_oneclass_dev_y[imdb_oneclass_dev_y == 0] = -1\n",
        "  imdb_oneclass_f1 = f1_score(imdb_oneclass_dev_y, imdb_oneclass_preds, pos_label = -1)\n",
        "\n",
        "  # Updating our model matrix\n",
        "  models[iteration] = [iteration, n, imdb_oneclass_f1]\n",
        "  print(models[iteration])\n",
        "  iteration = iteration + 1"
      ],
      "metadata": {
        "id": "SJ7i4UHtDQMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyperparameter configurations\n",
        "num_samples = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
        "num_models = len(num_samples)\n",
        "models = np.zeros(shape=(num_models, 3))\n",
        "\n",
        "# Isolation Forest hyperparameter tuning\n",
        "iteration = 0\n",
        "for n in num_samples:\n",
        "  # Training the Isolation Forest model\n",
        "  imdb_iforest = IsolationForest(max_samples = n, contamination = 0.25, random_state = 123)\n",
        "  imdb_iforest_model = imdb_iforest.fit(imdb_oneclass_train)\n",
        "\n",
        "  # Using our model to obtain predictions and compute the F1-score\n",
        "  imdb_iforest_preds = imdb_iforest_model.predict(imdb_dev)\n",
        "  imdb_iforest_f1 = f1_score(imdb_oneclass_dev_y, imdb_iforest_preds, pos_label = -1)\n",
        "\n",
        "  # Updating our model matrix\n",
        "  models[iteration] = [iteration, n, imdb_iforest_f1]\n",
        "  print(models[iteration])\n",
        "  iteration = iteration + 1"
      ],
      "metadata": {
        "id": "dY7MXdnRQOPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyperparameter configurations\n",
        "neighbours = [item for item in range(10, 36)]\n",
        "num_models = len(neighbours)\n",
        "models = np.zeros(shape=(num_models, 3))\n",
        "\n",
        "# Local Outlier Factor hyperparameter tuning\n",
        "iteration = 0\n",
        "for n in neighbours:\n",
        "  # Training the Local Outlier Factor model\n",
        "  imdb_lof = LocalOutlierFactor(n_neighbors = n, contamination = 0.25)\n",
        "  imdb_combined = vstack((imdb_oneclass_train, imdb_dev))\n",
        "\n",
        "  # Using our model to obtain predictions and compute the F1-score\n",
        "  imdb_lof_preds = imdb_lof.fit_predict(imdb_combined)\n",
        "  imdb_lof_preds = imdb_lof_preds[len(imdb_oneclass_train):]\n",
        "  imdb_lof_f1 = f1_score(imdb_oneclass_dev_y, imdb_lof_preds, pos_label = -1)\n",
        "\n",
        "  # Updating our model matrix\n",
        "  models[iteration] = [iteration, n, imdb_lof_f1]\n",
        "  print(models[iteration])\n",
        "  iteration = iteration + 1"
      ],
      "metadata": {
        "id": "UiDQc_1eVdzO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}